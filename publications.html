<!-- Publications page -->

<html>
<head>
    <title>Publications - Vardhan Dongre</title>
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="stylesheet" type="text/css" href="css/publications.css">
</head>

<body>
    <div class="content">
        <nav class="navigation">
            <a href="index.html">← Back to Home</a>
        </nav>
        
        <h1>Publications</h1>
        <h2>Research Papers & Publications</h2>
        
        <div class="publications-container">
            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations</h3>
                    <span class="publication-year">2025</span>
                </div>
                <div class="publication-authors">
                    <strong>Vardhan Dongre</strong>, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-Tür, Vikram S. Adve
                </div>
                <div class="publication-venue">
                    arXiv preprint
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2506.20100" class="publication-link" target="_blank">Paper</a>
                    <a href="https://github.com/MIRAGE-Benchmark/MIRAGE-Benchmark" class="publication-link" target="_blank">Code</a>
                    <a href="https://mirage-benchmark.github.io/" class="publication-link" target="_blank">Leaderboard</a>
                </div>
                <!-- <div class="publication-abstract">
                    We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning and decision-making in consultative interaction settings. Designed for the agriculture domain, MIRAGE captures the full complexity of expert consultations by combining natural user queries, expert-authored responses, and image-based context, offering a high-fidelity benchmark for evaluating models on grounded reasoning, clarification strategies, and long-form generation in a real-world, knowledge-intensive domain. Grounded in over 35,000 real user-expert interactions and curated through a carefully designed multi-step pipeline, MIRAGE spans diverse crop health, pest diagnosis, and crop management scenarios. The benchmark includes more than 7,000 unique biological entities, covering plant species, pests, and diseases, making it one of the most taxonomically diverse benchmarks available for vision-language models, grounded in the real world.
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions</h3>
                    <span class="publication-year">2025</span>
                </div>
                <div class="publication-authors">
                    Emre Can Acikgoz, Cheng Qian, Hongru Wang, <strong>Vardhan Dongre</strong>, Xiusi Chen, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur
                </div>
                <div class="publication-venue">
                    arXiv preprint
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2504.16939" class="publication-link" target="_blank">Paper</a>
                </div>
                <!-- <div class="publication-abstract">
                    Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. We systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following.
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">Better Slow than Sorry: Introducing Positive Friction for Reliable Dialogue Systems</h3>
                    <span class="publication-year">2025</span>
                </div>
                <div class="publication-authors">
                    Mert İnan, Anthony Sicilia, Suvodip Dey, <strong>Vardhan Dongre</strong>, Tejas Srinivasan, Jesse Thomason, Gökhan Tür, Dilek Hakkani-Tür, Malihe Alikhani
                </div>
                <div class="publication-venue">
                    Transactions of the Association for Computational Linguistics
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2501.17348" class="publication-link" target="_blank">Paper</a>
                    <a href="https://github.com/Merterm/Positive-Friction-Dialogue" class="publication-link" target="_blank">Code</a>
                </div>
                <!-- <div class="publication-abstract">
                    While theories of discourse and cognitive science have long recognized the value of unhurried pacing, recent dialogue research tends to minimize friction in conversational systems. Yet, frictionless dialogue risks fostering uncritical reliance on AI outputs, which can obscure implicit assumptions and lead to unintended consequences. To meet this challenge, we propose integrating positive friction into conversational AI, which promotes user reflection on goals, critical thinking on system response, and subsequent re-conditioning of AI systems. We hypothesize systems can improve goal alignment, modeling of user mental states, and task success by deliberately slowing down conversations in strategic moments to ask questions, reveal assumptions, or pause.
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents</h3>
                    <span class="publication-year">2024</span>
                </div>
                <div class="publication-authors">
                    <strong>Vardhan Dongre</strong>, Xiaocheng Yang, Emre Can Acikgoz, Suvodip Dey, Gokhan Tur, Dilek Hakkani-Tür
                </div>
                <div class="publication-venue">
                    International Workshop on Spoken Dialogue Systems Technology (Oral)
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2411.00927" class="publication-link" target="_blank">Paper</a>
                    <a href="https://github.com/vardhandongre/Respact" class="publication-link" target="_blank">Code</a>
                </div>
                <!-- <div class="publication-abstract">
                    Large language model (LLM)-based agents are increasingly employed to interact with external environments to solve user-provided tasks. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. ReSpAct demonstrates improved performance across diverse environments including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop).
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">Simulating User Agents for Embodied Conversational-AI</h3>
                    <span class="publication-year">2024</span>
                </div>
                <div class="publication-authors">
                    Daniel Philipov, <strong>Vardhan Dongre</strong>, Gokhan Tur, Dilek Hakkani-Tür
                </div>
                <div class="publication-venue">
                    NeurIPS Workshop on Open-World Agents
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2410.23535" class="publication-link" target="_blank">Paper</a>
                </div>
                <!-- <div class="publication-abstract">
                    Embodied agents designed to assist users with tasks must engage in natural language interactions, interpret instructions, execute actions, and communicate effectively to resolve issues. However, collecting large-scale, diverse datasets of situated human-robot dialogues to train and evaluate such agents is expensive, labor-intensive, and time-consuming. To address this challenge, we propose building a large language model (LLM)-based user agent that can simulate user behavior during interactions with an embodied agent in a virtual environment. Given a user goal (e.g., make breakfast), at each time step, the user agent may observe the robot actions or speak to either intervene with the robot or answer questions.
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">From Context to Action: Analysis of the Impact of State Representation and Context on the Generalization of Multi-Turn Web Navigation Agents</h3>
                    <span class="publication-year">2024</span>
                </div>
                <div class="publication-authors">
                    Nalin Tiwary*, <strong>Vardhan Dongre</strong>*, Sanil Arun Chawla, Ashwin Lamani, Dilek Hakkani-Tür
                </div>
                <div class="publication-venue">
                    NeurIPS Workshop on Open-World Agents
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2410.23555" class="publication-link" target="_blank">Paper</a>
                </div>
                <!-- <div class="publication-abstract">
                    Recent advancements in Large Language Model (LLM)-based frameworks have extended their capabilities to complex real-world applications, such as interactive web navigation. These systems, driven by user commands, navigate web browsers to complete tasks through multi-turn dialogues, offering both innovative opportunities and significant challenges. Despite the introduction of benchmarks for conversational web navigation, a detailed understanding of the key contextual components that influence the performance of these agents remains elusive. This study aims to fill this gap by analyzing the various contextual elements crucial to the functioning of web navigation agents.
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">Evaluating Uncertainty Quantification approaches for Neural PDEs in scientific applications</h3>
                    <span class="publication-year">2023</span>
                </div>
                <div class="publication-authors">
                    <strong>Vardhan Dongre</strong>, Gurpreet Singh Hora
                </div>
                <div class="publication-venue">
                    NeurIPS AI4Science, DLDEIII
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2311.04457" class="publication-link" target="_blank">Paper</a>
                </div>
                <!-- <div class="publication-abstract">
                    The accessibility of spatially distributed data, enabled by affordable sensors, field, and numerical experiments, has facilitated the development of data-driven solutions for scientific problems, including climate change, weather prediction, and urban planning. Neural Partial Differential Equations (Neural PDEs), which combine deep learning (DL) techniques with domain expertise (e.g., governing equations) for parameterization, have proven to be effective in capturing valuable correlations within spatiotemporal datasets. However, sparse and noisy measurements coupled with modeling approximation introduce aleatoric and epistemic uncertainties.
                </div> -->
            </div>
        </div>

        <div class="publications-note">
            <p>
                For a complete list of publications, please visit my 
                <a href="https://scholar.google.com/citations?user=sSt2OvIAAAAJ&hl=en&authuser=1">Google Scholar profile</a>.
            </p>
        </div>
    </div>
</body>
</html> 